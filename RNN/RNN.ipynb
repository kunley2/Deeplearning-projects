{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uKXCJGxDitL8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import keras.preprocessing.sequence\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading shakespeare text from google storage api to use for text prediction\n",
        "file = keras.utils.get_file('shakespeare.txt','https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5i5mdtxijZ7y",
        "outputId": "9455d80f-b3c3-432e-bb07-55ed3ede0682"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "1130496/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# opening the file and decoding it to use with python format\n",
        "text = open(file,'rb').read().decode(encoding='utf-8')\n",
        "print(text[:200])\n",
        "print(len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0YFHApLkd-c",
        "outputId": "57457387-e491-4d3a-b74f-b3b869c7e55f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you\n",
            "1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text)) # creating arranged unique words\n",
        "char2id = {l:no for no, l in enumerate(vocab)} # creating my bag of words(contains words and their index)\n",
        "id2char = np.array(vocab)\n",
        "print(vocab[:200])\n",
        "print(char2id)\n",
        "print(id2char)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8zpqA_9lKwC",
        "outputId": "5e4f3437-bceb-4549-a639-2b0eb14dbd7a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n",
            "['\\n' ' ' '!' '$' '&' \"'\" ',' '-' '.' '3' ':' ';' '?' 'A' 'B' 'C' 'D' 'E'\n",
            " 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W'\n",
            " 'X' 'Y' 'Z' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o'\n",
            " 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a function tha turn words to vector\n",
        "def text_to_int(text):\n",
        "  \"\"\" this function takes in texts and get the index of the word\"\"\"\n",
        "  word = [char2id[t] for t in text]\n",
        "  return np.array(word)\n",
        "def int_to_text(text):\n",
        "  try: \n",
        "    text.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(id2char[text])"
      ],
      "metadata": {
        "id": "svZWVTlqolFj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text2int = text_to_int(text)\n",
        "text2int[:15]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JorykusLp2eE",
        "outputId": "e8d54f15-3c3a-4fae-eb50-fdc19ce5dad1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(id2char.shape)\n",
        "print(int_to_text(text2int[:15]))\n",
        "print(vocab[18])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkj-ixAmqCDf",
        "outputId": "d8f1448c-fc1f-4ebd-8f52-cbfa4caa6320"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(65,)\n",
            "First Citizen:\n",
            "\n",
            "F\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  sequence length is the length of the training data to be used\n",
        "# what we want is that the output should contain the remaining input \n",
        "sequence_length = 100\n",
        "examples_per_epoch = len(text)//(sequence_length+1) # this shows the number of epochs for the text\n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text2int)\n",
        "# print(list(char_dataset.as_numpy_iterator()))\n",
        "sequence = char_dataset.batch(sequence_length+1,drop_remainder=True) # this create a batch with the number of sequence and drops the remaining\n"
      ],
      "metadata": {
        "id": "UcCWyk54ri4z"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to split the dataset into training and output\n",
        "def split_input(chunk):\n",
        "  \"\"\" this function takes in the input and remove the last word and output removes the first word and brings out everything\"\"\"\n",
        "  input = chunk[:-1]\n",
        "  output = chunk[1:]\n",
        "  return input,output\n",
        "\n",
        "dataset = sequence.map(split_input)\n",
        "dataset.take(2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozYfj4GCxojW",
        "outputId": "e90dbe36-bac9-4f2e-e6b1-a979a3a2339e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset shapes: ((100,), (100,)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for l,n in dataset.take(2):\n",
        "  print(f'input:{int_to_text(l)}','\\n')\n",
        "  print(f'output:{int_to_text(n)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynDULNbB13M_",
        "outputId": "9eafbc03-5f49-407c-95df-1c97e5a8f4a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input:First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n",
            "\n",
            "output:irst Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n",
            "input:are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you  \n",
            "\n",
            "output:re all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# time to train our model\n",
        "batch_size = 32\n",
        "vocab_size = len(vocab)\n",
        "rnn_units = 1024\n",
        "embedding_dim = 256\n",
        "\n",
        "train_data = dataset.shuffle(10000).batch(batch_size,drop_remainder=True)"
      ],
      "metadata": {
        "id": "qMrhsLl32nom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(vocab_size,batch_size,rnn_units,embedding_dim):\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(keras.layers.Embedding(vocab_size+1,embedding_dim,batch_input_shape=[batch_size,None]))\n",
        "  model.add(keras.layers.LSTM(1024,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'))\n",
        "  model.add(keras.layers.Dense(vocab_size))\n",
        "  return model\n",
        "\n",
        "model = create_model(vocab_size,batch_size,rnn_units,embedding_dim)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQyPeWty5rcp",
        "outputId": "196814a4-9549-4e6c-cee0-dae4bc23b5e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (32, None, 256)           16896     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (32, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense (Dense)               (32, None, 65)            66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,330,497\n",
            "Trainable params: 5,330,497\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.take(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Bxi1PDe58Av",
        "outputId": "4292161a-d323-4639-a8e1-e7a0094aa919"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset shapes: ((32, 100), (32, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# making predictions for the first batch of words\n",
        "# it shows 32 predictions of length 100 with total of 65 possible characters\n",
        "for training,testing in train_data.take(1):\n",
        "  predicted_val = model(training)\n",
        "  print(predicted_val.shape)\n",
        "  print(predicted_val[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5JGTOjgya8Z",
        "outputId": "cd8244f0-8317-4a26-f307-3a47b98d1d8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 100, 65)\n",
            "tf.Tensor(\n",
            "[[-0.00114096 -0.00890215 -0.00502146 ...  0.00069315 -0.00154731\n",
            "   0.00291908]\n",
            " [ 0.00216486 -0.01031527 -0.00512168 ...  0.00121373 -0.00083322\n",
            "   0.00103331]\n",
            " [ 0.00048114 -0.01574672  0.00030828 ...  0.00710972 -0.00610176\n",
            "   0.001842  ]\n",
            " ...\n",
            " [ 0.00932314 -0.00120466  0.00075075 ... -0.00386311 -0.00023657\n",
            "   0.00653169]\n",
            " [ 0.00741927 -0.00341913 -0.00326885 ... -0.00458156  0.00500496\n",
            "   0.00694083]\n",
            " [ 0.00572156 -0.00730574 -0.00156941 ...  0.00052032 -0.00093141\n",
            "   0.00639114]], shape=(100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampling_pred = tf.random.categorical(predicted_val[0],1)\n",
        "print(sampling_pred)\n",
        "sampled_indice = np.reshape(sampling_pred,(1,-1))[0]\n",
        "print(sampled_indice)\n",
        "print(sampled_indice.shape)\n",
        "print(int_to_text(sampled_indice))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meOVoiwIzVCs",
        "outputId": "fa2c9836-798b-4249-f7b3-482d63ce0654"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[ 8]\n",
            " [24]\n",
            " [26]\n",
            " [14]\n",
            " [59]\n",
            " [16]\n",
            " [ 6]\n",
            " [59]\n",
            " [64]\n",
            " [28]\n",
            " [ 1]\n",
            " [ 5]\n",
            " [ 0]\n",
            " [59]\n",
            " [13]\n",
            " [31]\n",
            " [10]\n",
            " [62]\n",
            " [10]\n",
            " [50]\n",
            " [ 6]\n",
            " [17]\n",
            " [62]\n",
            " [57]\n",
            " [ 0]\n",
            " [14]\n",
            " [19]\n",
            " [35]\n",
            " [51]\n",
            " [48]\n",
            " [13]\n",
            " [38]\n",
            " [ 0]\n",
            " [33]\n",
            " [59]\n",
            " [ 1]\n",
            " [ 7]\n",
            " [27]\n",
            " [26]\n",
            " [ 5]\n",
            " [51]\n",
            " [56]\n",
            " [54]\n",
            " [58]\n",
            " [ 0]\n",
            " [36]\n",
            " [49]\n",
            " [42]\n",
            " [52]\n",
            " [64]\n",
            " [15]\n",
            " [33]\n",
            " [31]\n",
            " [11]\n",
            " [42]\n",
            " [17]\n",
            " [57]\n",
            " [50]\n",
            " [56]\n",
            " [36]\n",
            " [54]\n",
            " [40]\n",
            " [ 2]\n",
            " [11]\n",
            " [14]\n",
            " [48]\n",
            " [27]\n",
            " [44]\n",
            " [17]\n",
            " [45]\n",
            " [16]\n",
            " [44]\n",
            " [59]\n",
            " [15]\n",
            " [10]\n",
            " [18]\n",
            " [ 9]\n",
            " [26]\n",
            " [18]\n",
            " [15]\n",
            " [ 7]\n",
            " [32]\n",
            " [16]\n",
            " [30]\n",
            " [30]\n",
            " [38]\n",
            " [59]\n",
            " [29]\n",
            " [58]\n",
            " [45]\n",
            " [28]\n",
            " [19]\n",
            " [36]\n",
            " [10]\n",
            " [32]\n",
            " [16]\n",
            " [28]\n",
            " [17]\n",
            " [39]\n",
            " [41]], shape=(100, 1), dtype=int64)\n",
            "[ 8 24 26 14 59 16  6 59 64 28  1  5  0 59 13 31 10 62 10 50  6 17 62 57\n",
            "  0 14 19 35 51 48 13 38  0 33 59  1  7 27 26  5 51 56 54 58  0 36 49 42\n",
            " 52 64 15 33 31 11 42 17 57 50 56 36 54 40  2 11 14 48 27 44 17 45 16 44\n",
            " 59 15 10 18  9 26 18 15  7 32 16 30 30 38 59 29 58 45 28 19 36 10 32 16\n",
            " 28 17 39 41]\n",
            "(100,)\n",
            ".LNBuD,uzP '\n",
            "uAS:x:l,Exs\n",
            "BGWmjAZ\n",
            "Uu -ON'mrpt\n",
            "XkdnzCUS;dEslrXpb!;BjOfEgDfuC:F3NFC-TDRRZuQtgPGX:TDPEac\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a loss function that helps calculate accuracy\n",
        "def loss(label,logit):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(label,logit,from_logits=True)"
      ],
      "metadata": {
        "id": "P5REYAqT16df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',loss=loss)"
      ],
      "metadata": {
        "id": "LdggRU3W4IOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating checkpoint to save the weight we used when training \n",
        "\n",
        "checkpoint_path = os.path.join('./training_checkpoint','ckpt_{epoch}')\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,save_weights_only=True)"
      ],
      "metadata": {
        "id": "t1ySIt-f4i8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_data,callbacks=[checkpoint_callback],epochs=25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDYpaCHv53HA",
        "outputId": "2a20e3bd-835a-46f5-dd4e-e044098c55b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "345/345 [==============================] - 14s 38ms/step - loss: 1.4558\n",
            "Epoch 2/25\n",
            "345/345 [==============================] - 14s 39ms/step - loss: 1.3702\n",
            "Epoch 3/25\n",
            "345/345 [==============================] - 15s 39ms/step - loss: 1.3110\n",
            "Epoch 4/25\n",
            "345/345 [==============================] - 15s 39ms/step - loss: 1.2626\n",
            "Epoch 5/25\n",
            "345/345 [==============================] - 15s 40ms/step - loss: 1.2197\n",
            "Epoch 6/25\n",
            "345/345 [==============================] - 15s 40ms/step - loss: 1.1757\n",
            "Epoch 7/25\n",
            "345/345 [==============================] - 15s 40ms/step - loss: 1.1305\n",
            "Epoch 8/25\n",
            "345/345 [==============================] - 15s 41ms/step - loss: 1.0850\n",
            "Epoch 9/25\n",
            "345/345 [==============================] - 15s 41ms/step - loss: 1.0374\n",
            "Epoch 10/25\n",
            "345/345 [==============================] - 15s 41ms/step - loss: 0.9909\n",
            "Epoch 11/25\n",
            "345/345 [==============================] - 15s 41ms/step - loss: 0.9440\n",
            "Epoch 12/25\n",
            "345/345 [==============================] - 15s 41ms/step - loss: 0.8993\n",
            "Epoch 13/25\n",
            "345/345 [==============================] - 15s 41ms/step - loss: 0.8575\n",
            "Epoch 14/25\n",
            "345/345 [==============================] - 15s 41ms/step - loss: 0.8173\n",
            "Epoch 15/25\n",
            "345/345 [==============================] - 15s 41ms/step - loss: 0.7827\n",
            "Epoch 16/25\n",
            "345/345 [==============================] - 15s 41ms/step - loss: 0.7506\n",
            "Epoch 17/25\n",
            "345/345 [==============================] - 15s 41ms/step - loss: 0.7237\n",
            "Epoch 18/25\n",
            "345/345 [==============================] - 16s 43ms/step - loss: 0.6968\n",
            "Epoch 19/25\n",
            "345/345 [==============================] - 16s 42ms/step - loss: 0.6750\n",
            "Epoch 20/25\n",
            "345/345 [==============================] - 15s 41ms/step - loss: 0.6554\n",
            "Epoch 21/25\n",
            "345/345 [==============================] - 15s 41ms/step - loss: 0.6399\n",
            "Epoch 22/25\n",
            "345/345 [==============================] - 16s 42ms/step - loss: 0.6263\n",
            "Epoch 23/25\n",
            "345/345 [==============================] - 16s 41ms/step - loss: 0.6127\n",
            "Epoch 24/25\n",
            "345/345 [==============================] - 15s 41ms/step - loss: 0.6017\n",
            "Epoch 25/25\n",
            "345/345 [==============================] - 15s 41ms/step - loss: 0.5906\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7620571950>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the model again but with one batch\n",
        "model = create_model(vocab_size,1,rnn_units,embedding_dim)\n",
        "model.load_weights(tf.train.latest_checkpoint('./training_checkpoint'))\n",
        "model.build(tf.TensorShape([1,None]))"
      ],
      "metadata": {
        "id": "a5IfBdup7SwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model,starting_string):\n",
        "  # numbers of character to generate\n",
        "  number_to_generate = 200\n",
        "  # converts the text to vector\n",
        "  input = text_to_int(starting_string)\n",
        "  input = tf.expand_dims(input,0)\n",
        "  generated_text = []\n",
        "  # reset the model so i can pass one value at the last layer\n",
        "  model.reset_states()\n",
        "  for i in range(number_to_generate):\n",
        "    pred = model(input)\n",
        "    pred = tf.squeeze(pred,0)\n",
        "    pred_ind = tf.random.categorical(pred,1)[-1,0].numpy()\n",
        "    # passing the predicted value as input\n",
        "    input =  tf.expand_dims([pred_ind],0)\n",
        "    generated_text.append(int_to_text(pred_ind))\n",
        "  return (starting_string + ''.join(generated_text))\n"
      ],
      "metadata": {
        "id": "hZxKoF_z-EDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "starting_string = input('type in a word:')\n",
        "print(generate_text(model,starting_string))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aMhfjypDdf5",
        "outputId": "6d0c042b-9267-4e0b-ae4b-5a62dba0b29a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type in a word:kunle\n",
            "kunled!\n",
            "\n",
            "LADY CAPULET:\n",
            "I would it gentlemen. More lord to Lord Hunger:\n",
            "You stoop it, truly if thou darest.\n",
            "\n",
            "KING EDWARD IV:\n",
            "Take him up in Plantagenet. A plague o' both your house:\n",
            "I'll look pale\n",
            "To take o\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "x17Kw88QDrVm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}